# NVIDIA NIM Deployment Options for FHIR GraphRAG

## Overview

NVIDIA NIM can be deployed in two fundamentally different ways:
1. **NVIDIA API Cloud** - Hosted inference via API calls (like OpenAI)
2. **Self-Hosted NIM** - Run NIM containers on your own GPU infrastructure

## Option 1: NVIDIA API Cloud (Recommended for Development)

### What It Is
- NVIDIA hosts the models on their infrastructure
- You make API calls over HTTPS
- Pay per API call (similar to OpenAI pricing)
- **No GPU needed locally**

### Architecture
```
Your MacBook/EC2 Instance
  â””â”€â†’ HTTPS API Call
       â””â”€â†’ NVIDIA Cloud (hosted models)
            â””â”€â†’ Returns embeddings
```

### Pros
- âœ… **No GPU required** - works on MacBook, standard EC2
- âœ… **Fast setup** - just need API key
- âœ… **Zero infrastructure** - no Docker, Kubernetes, GPU drivers
- âœ… **Auto-scaling** - NVIDIA handles load
- âœ… **Always updated** - latest model versions
- âœ… **Low startup cost** - pay only for what you use

### Cons
- âŒ Data leaves your infrastructure (sent to NVIDIA cloud)
- âŒ Per-query costs (can add up at scale)
- âŒ Network latency (API call overhead)
- âŒ Dependent on NVIDIA service availability
- âŒ Rate limits on free tier

### Cost Model
**Free Tier:**
- Limited requests/day (check build.nvidia.com for current limits)
- Good for: Development, testing, small datasets

**Paid Tier:**
- ~$0.0002 per 1K tokens (estimate - check current pricing)
- Example: 10K queries/day Ã— 100 tokens avg = 1M tokens/day = $0.20/day = $6/month

### When to Use
- âœ… Development and prototyping
- âœ… Low-volume production (<1K queries/day)
- âœ… Quick proof-of-concept
- âœ… Testing before committing to infrastructure
- âœ… **Phase 3 of our implementation**

### Setup
1. Get API key from build.nvidia.com
2. Install: `pip install langchain-nvidia-ai-endpoints`
3. Use in code:
```python
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings
embeddings = NVIDIAEmbeddings(model="nvidia/nv-embedqa-e5-v5")
vector = embeddings.embed_query("chest pain")
```

---

## Option 2: Self-Hosted NIM (For Production)

### What It Is
- Download NIM Docker containers from NVIDIA
- Run on your own GPU infrastructure (AWS, GCP, Azure, on-prem)
- Models run locally on your GPUs
- You manage infrastructure and scaling

### Architecture Options

#### 2a. AWS EC2 with GPU (Simple)

```
AWS EC2 Instance (g5.xlarge)
  â”œâ”€â†’ NVIDIA A10G GPU (24GB VRAM)
  â”œâ”€â†’ Docker Engine
  â”‚    â””â”€â†’ NIM Container
  â”‚         â””â”€â†’ NV-EmbedQA-E5-v5 model
  â””â”€â†’ Your FHIR App (localhost connection)
```

**Setup Steps:**
1. Launch EC2 instance (g5.xlarge, g4dn.xlarge, or p3.2xlarge)
2. Install NVIDIA drivers and Docker
3. Pull NIM container: `docker pull nvcr.io/nvidia/nv-embedqa-e5-v5`
4. Run container with GPU support
5. Connect your app to localhost:8000

**Pros:**
- âœ… Simple architecture
- âœ… Data stays in your infrastructure
- âœ… Unlimited queries (limited by GPU capacity)
- âœ… Low latency (local inference)
- âœ… Full control over model versions

**Cons:**
- âŒ Fixed cost (instance runs 24/7)
- âŒ Manual scaling (need multiple instances for high load)
- âŒ You manage GPU drivers, Docker, monitoring
- âŒ No auto-scaling

**Cost:**
- g5.xlarge: $1.006/hour = $24/day = $720/month
- g4dn.xlarge: $0.526/hour = $12.62/day = $380/month
- Break-even vs API: ~10K-20K queries/day

**Instance Types:**
| Instance | GPU | VRAM | $/hour | Best For |
|----------|-----|------|--------|----------|
| g4dn.xlarge | T4 | 16GB | $0.526 | Text embeddings |
| g5.xlarge | A10G | 24GB | $1.006 | Text + Vision |
| g5.2xlarge | A10G | 24GB | $1.212 | High throughput |
| p3.2xlarge | V100 | 16GB | $3.06 | Vision models |

#### 2b. AWS EKS with GPU Nodes (Production-Grade)

```
AWS EKS Cluster
  â”œâ”€â†’ Control Plane ($73/month)
  â”œâ”€â†’ GPU Node Group (g5.xlarge instances)
  â”‚    â”œâ”€â†’ NIM Pods (auto-scaling)
  â”‚    â””â”€â†’ NVIDIA GPU Operator
  â”œâ”€â†’ Application Node Group (t3.large instances)
  â”‚    â””â”€â†’ FHIR GraphRAG App Pods
  â”œâ”€â†’ Application Load Balancer
  â””â”€â†’ Auto Scaling Groups
```

**Setup Steps:**
1. Create EKS cluster with GPU node group
2. Install NVIDIA GPU Operator (manages drivers)
3. Deploy NIM as Kubernetes Deployment
4. Configure Horizontal Pod Autoscaler
5. Set up Ingress/Load Balancer

**Pros:**
- âœ… Auto-scaling (scale GPU pods based on load)
- âœ… High availability (multi-instance)
- âœ… Rolling updates (zero-downtime deployments)
- âœ… Enterprise-grade orchestration
- âœ… Resource optimization (pack multiple services)

**Cons:**
- âŒ Complex setup (Kubernetes expertise required)
- âŒ Higher baseline cost (EKS + minimum nodes)
- âŒ More moving parts to manage
- âŒ Longer time to implement

**Cost:**
- EKS Control Plane: $73/month
- 2Ã— g5.xlarge nodes: $1,440/month
- Load Balancer: $20/month
- **Total: ~$1,533/month**

**When to Use:**
- âœ… High query volume (>50K/day)
- âœ… Variable load patterns
- âœ… Multiple NIM models
- âœ… Enterprise production requirements

#### 2c. AWS SageMaker (Managed Inference)

```
AWS SageMaker
  â”œâ”€â†’ Model Registry (store NIM models)
  â”œâ”€â†’ Endpoint (managed inference)
  â”‚    â””â”€â†’ GPU instances (auto-scaling)
  â””â”€â†’ Monitoring (CloudWatch)
```

**Pros:**
- âœ… Fully managed by AWS
- âœ… Built-in auto-scaling
- âœ… Integrated monitoring
- âœ… Pay-per-inference pricing option

**Cons:**
- âŒ May not support all NIM containers
- âŒ Less control than EKS
- âŒ Potential vendor lock-in

**Cost:**
- ml.g5.xlarge: $1.408/hour
- Plus data transfer costs

---

## Decision Matrix

### For Phase 3 (Text Embeddings - NOW)
**Recommendation: NVIDIA API Cloud**

| Requirement | API Cloud | EC2 GPU | EKS |
|-------------|-----------|---------|-----|
| Quick setup | âœ… Minutes | âš ï¸ Hours | âŒ Days |
| No GPU needed | âœ… Yes | âŒ No | âŒ No |
| Cost to start | âœ… Free tier | âŒ $720/mo | âŒ $1500/mo |
| Good for 51 docs | âœ… Perfect | âš ï¸ Overkill | âŒ Overkill |

### For Phase 4+ (Vision + Production)
**Consider: Self-Hosted if query volume justifies**

| Query Volume | Recommendation | Estimated Cost |
|--------------|----------------|----------------|
| <1K/day | API Cloud | $1-10/month |
| 1K-10K/day | API Cloud or EC2 | $50-100/month |
| 10K-50K/day | EC2 GPU | $400-800/month |
| >50K/day | EKS | $1500+/month |

---

## Hybrid Approach (Recommended Strategy)

**Phase 3 (Development):**
â†’ Use NVIDIA API Cloud
- Validate NIM embeddings work
- Test on 51 DocumentReferences
- Measure accuracy improvement
- **Cost: ~$5/month**

**Phase 4 (Scale Testing):**
â†’ Still use API Cloud initially
- Test with 10K patient dataset
- Measure query volume and costs
- Calculate break-even point
- **Cost: ~$50-100/month**

**Production Decision:**
â†’ Choose based on actual usage
- If <10K queries/day â†’ Stay on API Cloud
- If >10K queries/day â†’ Move to EC2 GPU
- If >50K queries/day â†’ Consider EKS

---

## Implementation Plan Update

### Immediate (Phase 3): NVIDIA API Cloud âœ…
```bash
# No AWS setup needed!
export NVIDIA_API_KEY="nvapi-xxx"
pip install langchain-nvidia-ai-endpoints
python src/setup/nim_text_vectorize.py
```

### Later (Phase 4+): Evaluate Self-Hosting
```bash
# If query volume justifies:
# 1. Launch AWS EC2 g5.xlarge
# 2. Install NVIDIA drivers + Docker
# 3. Pull NIM container
# 4. Deploy and benchmark
```

---

## Security Considerations

### API Cloud
- âœ… Data encrypted in transit (HTTPS)
- âš ï¸ Data processed on NVIDIA servers
- âœ… SOC 2 compliant (NVIDIA)
- âš ï¸ Check compliance requirements (HIPAA, etc.)

### Self-Hosted
- âœ… Data never leaves your infrastructure
- âœ… Full control over security
- âœ… HIPAA compliant (if configured properly)
- âš ï¸ You responsible for security patches

---

## Recommendation for FHIR GraphRAG Project

**Start with NVIDIA API Cloud** because:
1. **Phase 3 focus**: Text embeddings on 51 documents
2. **MacBook friendly**: No GPU setup needed
3. **Fast iteration**: Validate approach quickly
4. **Low cost**: ~$5-10/month for testing
5. **Later optimization**: Move to self-hosted if needed

**Transition to self-hosted when:**
1. Query volume >10K/day
2. Cost exceeds $100/month on API
3. Data compliance requires on-prem
4. Vision models need local GPUs

---

## Next Steps

### Today: Get Started with API Cloud
1. Get NVIDIA API key from build.nvidia.com
2. Test API access
3. Implement NIM text embeddings
4. Re-vectorize 51 DocumentReferences
5. Measure accuracy improvement

### Later: Evaluate Self-Hosting
1. Monitor query volume over time
2. Calculate actual API costs
3. Compare to EC2 GPU costs
4. Make data-driven decision

**Bottom line: Use API Cloud now, optimize later!** ğŸš€
